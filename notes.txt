Structure of JIT kernel source (CPU):

Enums:
GB_jit_family
    - Enum mapping family name (i.e. GB_jit_ewise_family)
    - Family of methods supported by the JIT
    - Branches off into individual kernels/kcodes
GB_jit_kcode:
    - Enum mapping kcode names (i.e. GB_JIT_KERNEL_ROWSCALE) to a kcode number
    - CUDA kcodes: >= 1000

Structs:
GB_jit_encoding
    - Tracks code, kcode, and suffix_len of a problem instance
    - code: pinpoints specific instance of problem
    - kcode: pinpoints which kernel
    - suffix_len: length of kname suffix (may be needed for user-defined types/operators)

Process:
1. Encodifying:
    - Happens in the jit entry point (*_jit functions)
        - Calls GB_encodify_{family_name}
        - Encodify takes the objects needed for encodifying that family, and the kcode
            - For reduce, the input objects would be a monoid and input matrix
            - For select, right now needs output characteristics, indexunaryop used for selecting, input matrix, and some other things
            - The actual output matrix is not always needed, but may be in certain cases to determine if e.g. atomics are available
        - Encodify gives a GB_jit_encoding, the kname suffix, and returns a hash of the entire encoding
            - The hash will be invalid (UINT64_MAX) if the encoding is invalid

    a. Enumifying:
        - Happens as a step during encodifying
        - enumify function exists for all jit families, but extra enumifies may exist where
          needed in order to compute family-level enumifies (for example, enumify_sparsity)
        - Enumify takes the objects needed to be enumified
        - Enumify gives the rcode. Some enumifies don't use too many bits for their rcodes. 
          Others (like mxm) use a lot.

2. GB_jitifyer_load
    - Main entry point to the JIT
    - Takes the family, kname, encoding hash, encoding, kname suffix
        - Also takes 5 objects: A semiring, monoid, operator, and 3 types
            - These are optional, and depend on the kernel being jitifyed. 
    - Gives the function to be called

    a. Preliminary steps
        - Determines if JIT is enabled (GB_jit_control)
            - JIT can be in 5 modes: GxB_JIT_{OFF | PAUSE | RUN | LOAD | ON}
            - If in RUN mode and not jitting a user type/op, attempts to lookup in table and run
            - Load vs. looking up: Load means dlopen the library
    
    b. Goes to GB_jitifyer_worker

3. GB_jitifyer_worker
    - Worker function that does hash table lookup, followed by kernel load/compile
    - Takes same input as GB_jitifyer_load
    - Guarded by critical section: only one thread per process may invoke at a time
        - Further, since load/compile alters the filesystem, mutual exclusion must be
          enforced across processes. For this reason, a file lock is used.
    
    a. Performs hash table lookup for function
        - Quits early if succeeds
    b. Calls macrofy_name to get full kernel_name
    c. File locking
    d. Calls GB_jitifyer_load_worker, which finishes the load/compile step
    

4. GB_jitifyer_load_worker
    - Does main load/compile work, inside both intra-process (critical section) and inter-process (file lock)
      guards
    a. Checks if kernel is already compiled
    b. 

Macrofying:
    1. macrofy_preface
        - Prints version, date, dislaimer header
        - Includes GB_jit_kernel.h
            - Setup for kernels
            - Includes GB_Template.h which includes all main GraphBLAS headers
            - undef's structure access macros to be redefined below
    2. macrofy_family
        - Main part of macrofy
        - Prints the family-relevant macros
        - Starts by switching over kernel family (apply, reduce, mxm, ...)
            - Low-level macrofy extracts problem characteristics from
            encoding code            
            - Uses macrofy_input to print macros for matrix access
    3. Handle pre-jit case:
        #ifndef GB_JIT_RUNTIME ...
        #endif

    4. #include kernel source (named GB_jit_kernel_{kname}.c)
        - Will include templates within
        - In Source/JitKernels and Source/Template respectively
    5. macrofy_query
        - Writes the query function that returns the problem characteristics
        using the given encoding

Old GPU JIT notes:
- Entry point: cpp file like GB_AxB_dot3_cuda_jit.cpp. This is analogous to a *_jit.c file for the CPU jit
    - Uses a problem factory to store problem details (i.e. *reduce_factory, *mxm_factory). This is a FileDesc.
        - More specifically, the {reduce_factory, mxm_factory, ...} etc. function is called with the problem args to "build" the details
        - Uses enumify internally to build the sr_code, source filename with the format "GB_{family}_{sr_code}.h"
        - Exposes a macrofy function, that given a FILE* fp, will use the CPU macrofy to dump the family macros (does NOT include the preface, query, etc.)
    - Uses a jit factory for things like checking for the function in cache and launching the function
        - *cuda_common_jitFactory.hpp: Has compiler flags
- Unlike CPU side, the main code is not in the Templates folder but JitKernels



New GPU JIT notes:
- Moving away from NVIDIA jitifyer, integrating with GraphBLAS JIT
- 3-stage structure:
    1. Entry point (GB_cuda_{problem}.cpp): decides initial things (such as if atomics are available (has_cheeseburger))
    2. JIT entry (GB_cuda_{problem}_jit.cpp): similar in structure to the CPU jit entries. Computes encoding, calls jitifyer_load, calls returned
       function
    3. JitKernel code (JitKernels/GB_jit_kernel_cuda_{problem}.cu): What gets run by the jit. Unlike NVIDIA jitifyer, has host code in it (proto function)
       that gets returned by the jitifyer. Advantage of the host proto function: can manage multiple kernel launches in a single run


Progress notes:
Kernel development notes:
- Useful guide: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html
- Good tips: https://on-demand.gputechconf.com/gtc/2017/presentation/s7122-stephen-jones-cuda-optimization-tips-tricks-and-techniques.pdf
- Fork GraphBLAS, establish personal branch
- Start with rowscale
    - Understand how rowscale works on CPU
        - Mainly need to analyze rowscale template code
    - Understand structure of CUDA kernel (entry point, jit point, etc.)

- Overall plan:
    - Start working on template code
    - Work backwards to entry point/jit entry

- Rowscale notes:
    - Do NOT use GB_PARTITION to divide work for threads
        - Do not want consecutive threads operating on blocks of data
          with large separation between them, since they will reside on the
          same warp and cause locality issues. This means that each thread should
          "jump" through the data by the grid/block size.

- Colscale notes:
    - Key idea: At the kernel level, generally all matrices are viewed as being stored by column
    - Issue with colscale: Cannot easily identify the column that an entry belongs to
    - cuda_ek_slice: slices e entries among ntasks tasks
        - Applies only for sparse/hypersparse formats
        - Example: CUDA/Template/GB_cuda_jit_AxB_dot3_dense_phase1
        - Core idea:
            - Divide work into chunks (size of chunk given by max_pchunk)
            - One threadblock may own multiple chunks; staggered
                - blk 0 will start at chunk 0, then chunk gridDim, then gridDim * 2, ...
                - hybrid approach for work distribution; thread allocation contiguous within chunks but jumps across chunks
            
        - 3 components:
            1. cuda_ek_slice
                - Takes Ap, anvec, anz, pfirst (first entry of work for this threadblock),
                   max_pchunk
                - Computes ks for all entries in the given chunk [pfirst, min(pfirst + max_pchunk - 1, anz - 1)]
            2. cuda_ek_slice_setup
                - Gives the kfirst/klast estimates, slope
                    - slope: rate at which k changes w.r.t one step of p
                        - sparser matrices -> bigger slope (intuition)                    
            3. cuda_ek_slice_entry
                - Uses estimates + slope from ek_slice_setup to get an exact
                  k-value for a p-value

- Apply notes:
    - Can use a unary op, index unary op, or binary op
    - Entry point: GB_apply
    - Main work: GB_apply_op
        - 5 cases:
            1. Positional op (not user-defined)
                - Handled by templates (NOT JIT)
                - Ideally, should be handled by JIT. Problem: positional binops not jitted
                    - Solution: Convert positional binops to positional unops
                - Overview of all ops:
                    a. unops (GrB_UnaryOp):
                        - Have both positional/non-positional
                        - Non-positional are plain z = f(x)
                        - Positional: GxB_POSITION* (does NOT depend on x)
                            - f(x) = i/j
                    b. binops (GrB_BinaryOp):
                        - Like unop, has positional/non-positional
                        - Non-positional: are plain z = f(x, y)
                        - Positional: GxB_{FIRST | SECOND}* (does NOT depend on x, y)
                            - Always gives row/col, based on I or J name
                    c. indexunaryops (GrB_IndexUnaryOp)
                        - Part of spec, fulfill the role of positional unops
                        - While positional unops do NOT consider the input,
                          IndexUnaryOps can (the "thunk")
                          - thunk adds a constant offset to result
                    

            2. Iso op
                - When C is iso. Handled by iso kernel (NOT JIT)
            3. Unary op (not positional)
                - Tries factory kernel, then CUDA JIT, then JIT then generic
            4. Binary op (not positional)
                - bind1st/bind2nd: is the scalar on left or right
                - Same process as (3)
            5. User-defined indexunaryop
                - Need to handle all "depends on" cases (i, j, x, y) since
                  user-defined

