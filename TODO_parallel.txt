GraphBLAS/TODO_parallel.txt: Here are a list of the Source/*.c files that need
to be done in parallel.  I've roughly sorted them by priority and partly by the
low-hanging fruit (some will be quick to do).  You can find them all with:

cd GraphBLAS/Source
grep -r PARALLEL .

There are just (!) 36 files to consider in all of GraphBLAS.  The other files
either call these, or do O(1) or O(log(..)) work and won't be done in parallel.

Testing the parallel codes:  Most of my test suite is done through MATLAB
mexFunctions in the GraphBLAS/Test directory, but that could be a hassle.
Better would be to use the algorithms and main programs in LAGraph (BFS for
now), GraphBLAS/Demo (BFS, MIS, PageRank, triangle counting) and
GraphBLAS/Extras (triangle counting, k-truss, and all-k-truss).  The k-truss
and all-k-truss do lots of matrix-matrix multiplies.  Pagerank is a good
test for mxv via "*FINE" slice methods.

We should probably write some unit tests for simpler functions like sorting and
transpose and such.  LAGraph would be the thing to use since it has a nice
LAGraph_mmread and LAGraph_mmwrite pair of functions.  Main programs in
GraphBLAS/Demo are more tedious without these nice functions.
For the parallel qsort, use LAGraph/BuildMatrix/btest to test the parallel
performance.

See the new GBI_parallel_for_each_vector(A) iterator.
The more complex iterators (GBI2, GBI2s, GBI3, GBI3s) can be ignored for now;
they iterate over a set union of 2 or more lists.  They are currently not
easily parallelizable and need to be replaced with the single GBI_ iterator if
the loop is to be parallelized.  See for example the new GB_add.c.

All parallel methods need tuning.  Right now, I do not reduce nthreads
for smaller problems, and so they can see significant slowdowns
(C=A*B when the matrices are 16-by-16 with 16 threads is very slow).
All parallel regions use the num_threads(nthreads) construct.  Need
to write this instead:

    int nth = GB_nthreads_workload (c*work, nthreads) ;
    #pragma omp parallel for num_threads(nth)
    for ( ... )

where "work" is the total work of the parallel for loop or region.
It should be O(actual work), with a constant c that will vary per
for loop.  Then decide on a chunk size (perhaps in a global parameter)
that does something like:

    int GB_nthreads_workload (double work, int nthreads)
    {
        return (max (nthreads, 1 + floor (work / chunk))) ;
    }

----------------------------------------------------------------------
DONE or mostly done:
----------------------------------------------------------------------

GB_AxB_parallel.c: much is done: mxm nicely parallel for large matrices,
    and decent parallelism for mxv when using the dot product.  This is where
    we do C=A*B, for GrB_mxm, GrB_mxv, and GrB_vxm.  *FINE slice methods
    written but need tuning.  Need better fine-grain methods for mxv and vxm:
    do symbolic analysis phase (column nnz count), and then compute C in place
    without the final copy.  See for example *dot2.c. Need to do the same for
    Gustavson's method and for the Heap method.  Need to add the SLICE methods
    to the descriptor.  Need to write a hash-based method.

GB_AxB_flopcount.c: mostly done: reasonably implemented but needs a little work.
    Used by C=A*B for the parallel case, simple for all loop.  Note however the
    early exit, but this is only when nthreads == 1.  Currently, one thread has
    a 'continue' statement but if any thread terminates then all threads could
    break early.

GB_transplant.c: mostly done
    utility function used by almost all of GraphBLAS.  Does all its work in the
    parallel GB_memcpy.  But could use OpenMP tasks to do all GB_memcpy's in
    parallel.

GB_cast_array.c: done
    Typecasts the numerical values, so used everywhere.

GB_add.c: done, for now, except for final prune of C->h.   Needs a better
    method to slice vectors (see GB_reduce_each_vector and extend this to
    GB_add).  Does the work for GrB_*_eWiseAdd, and the accumulator operator.

GB_emult.c: done, except for final prune of C->h.  See also the future work
    for GB_add.  Does "C=A.*B" for GrB_*_eWiseMult_*.

GB_cumsum.c: done, but not as fast as I think it should be.
    a parallel cumulative-sum.  Used many places.  Note the two variants.
    Performance of the parallel case is at most 2x speedup, unless the array is
    already in L3 cache.  Needs more work.

GB_transpose.c: done, except for the qsort
    uses two methods, a qsort-based method and a bucket sort.

GB_transpose_ix, GB_transpose_op, GB_transpose_bucket: done

GB_reduce_to_scalar.c: done
    Does the work for GrB_reduce, to a scalar.  Simple reduction for loop.

GB_extractTuples.c: done, except tasks can be used to extract I,J,X in parallel
    does GrB_*_extractTuples.

GB_apply_op.c: done
    Applies a unary operator for GrB_*_apply.

GB_wait.c: done, except it will be updated when GB_add can tolerate zombies
    on input

GB_dup.c:  done.  4 parallel memcpy's, but these can also be 4 tasks.
    does the work for GrB_*_dup.  Easy to parallelize.

GB_build.c, GB_builder: done
    does GrB_*_build, GB_wait, GrB_reduce_to_vector (transposed),
    and GrB_transpose (when the qsort method is used).

GB_reduce_to_vector.c: done
    does the work for GrB_*_reduce (to a vector)

----------------------------------------------------------------------
in progress
----------------------------------------------------------------------

Template/GB_qsort_template.c: IN PROGRESS
    Needs a way to set a larger leaf task size.
    Does lots of work, everywhere, for many GraphBLAS operations.
    Odd segfault with 20 threads.  see "TODO"

----------------------------------------------------------------------
not started
----------------------------------------------------------------------

GB_resize.c:  see GB_delete_zombies.  Uses Template/GB_prune_inplace.c, which
    will be deleted and replaced with a template also used for
    GB_delete_zombies

GB_select.c:
    does the work for my GxB_*_select operations.  Use a template for this
    and GB_delete_zombies.

GB_to_nonhyper.c, GB_to_hyper.c:
    converts to/from hypersparse and standard.  Important utility, but does
    only O(n) work if n = # vectors in the matrix.  Parallel speedup will be
    low.

GB_mask.c:
    Does the C<M>=result for all operations.  Important function.  Parallel
    method would look like GB_add.

GB_nvec_nonempty.c:
    counts # of non-empty vectors.  simple parallel reduction.
    Small utility routine used everywhere, but doesn't do a lot of work
    so lower priority.

GB_kron_kernel.c:
    should be easy to do.  Could just precompute the size of each column
    of the output matrix, quickly, and then to a parallel cumulative sum.
    Then fill the output.  But not as essential as the methods above.

GB_assign.c:
    Does the work of GrB_*_assign.
    This will be hard, but not impossible.
    Most of the work is in GB_subassign_kernel

Template/GB_subref_template.c:
    Does the work for A=C(I,J).  The list J can be partitioned and this can
    be called in parallel to create independent submatrices, like
    A1 = C(I,J1) where J1 is the first set of columns in J.
    Then the results can be concatenated together, like C=A*B.

GB_subassign_kernel.c:
    This does C(I,J)=A.  Huge file, but could be parallel since the main pass
    does not modify the pattern of C at all, except to kill entries by making
    them zombies.  If a new entry is to be added, it gets put into a list of
    pending tuples.  Each thread could make its own list, and the lists could
    be combined at the end.

GB_I_inverse.c, GB_ijproperties.c, GB_ijsort.c:
    utility functions for assign and extract (A=C(I,J) and C(I,J)=A).
    I_inverse is hard to parallelize.
    GB_ijsort: deletes duplicates, see also GB_builder.  This is only used in
    GB_assign, for scalar expansion and for the C_replace_phase, and only when
    I and/or J are lists (not GrB_ALL, nor lo:inc:hi).

GB_is_diagonal: determines if A is diagonal, for C=A*B.  Very little work to
    do, and can exit early as soon as it finds a vector k so that A(:,k)
    contains anything more than a single A(k,k) entry.  So it really needs
    a parallel early-exit.

