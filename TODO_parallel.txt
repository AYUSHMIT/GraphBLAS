GraphBLAS/TODO_parallel.txt: Here are a list of the Source/*.c files that need
to be done in parallel.  I've roughly sorted them by priority and partly by the
low-hanging fruit (some will be quick to do).  You can find them all with:

cd GraphBLAS/Source
grep -r PARALLEL: .

Testing the parallel codes:  Most of my test suite is done through MATLAB
mexFunctions in the GraphBLAS/Test directory.  The mexFunctions link against
the GraphBLAS library, so do a "make install" and then use Test/testall.m
in MATLAB.

Alternatively: use the algorithms and main programs in LAGraph (BFS for now),
GraphBLAS/Demo (BFS, MIS, PageRank, triangle counting) and GraphBLAS/Extras
(triangle counting, k-truss, and all-k-truss).  The k-truss and all-k-truss do
lots of matrix-matrix multiplies.  Pagerank is a good test for mxv via "*FINE"
slice methods.  For the parallel qsort, use LAGraph/BuildMatrix/btest to test
the parallel performance.  See also Test/DNN.

A few methods use the GBI_parallel_for_each_vector(A) iterator, but most use
their own parallel for loop structure.  The more complex iterators (GBI2,
GBI2s, GBI3, GBI3s) can be ignored for now; they iterate over a set union of 2
or more lists.  They are currently not easily parallelizable and need to be
replaced with simpler loops to parallelize them.

All parallel methods need tuning and benchmarking.

----------------------------------------------------------------------
done:
----------------------------------------------------------------------

GB_transpose_ix
GB_transpose_op
GB_transpose_bucket
GB_extractTuples
GB_apply_op
GB_dup
GB_build
GB_builder
GB_reduce_to_vector
GB_select
GB_selector
GB_transplant
GB_cast_array
GB_nvec_nonempty
GB_memcpy
GB_AxB_colscale
GB_AxB_rowscale
GB_to_nonhyper
GB_to_hyper
GB_is_diagonal
GB_cumsum
GB_kron_kernel
GB_emult
GB_mask
GB_hyper_prune
GB_ijproperties

----------------------------------------------------------------------
mostly done:
----------------------------------------------------------------------

GB_AxB_parallel: much is done: mxm nicely parallel for large matrices,
    and decent parallelism for mxv when using the dot product.  This is where
    we do C=A*B, for GrB_mxm, GrB_mxv, and GrB_vxm.  *FINE slice methods are
    written but need tuning.  Need better fine-grain methods for mxv and vxm:
    do symbolic analysis phase (column nnz count), and then compute C in place
    without the final copy.  See for example *dot2. Need to do the same for
    Gustavson's method and for the Heap method.  Need to add the SLICE methods
    to the descriptor.  Need to write a hash-based method.

GB_AxB_flopcount: mostly done: reasonably implemented but needs a little work.
    Used by C=A*B for the parallel case, simple for all loop.  Note however the
    early exit, but this is only when nthreads == 1.  Currently, one thread has
    a 'continue' statement but if any thread terminates then all threads could
    break early.

GB_add: done, except for the merge when both A and B are hypersparse.
    Modify to tolerate zombies on input, to make GB_wait faster?

GB_wait: done, except it might be updated when GB_add can tolerate zombies
    on input

GB_transpose: done, except for the qsort.  uses two methods, a qsort-based
    method and a bucket sort.  Once the qsort is done, need to tune the auto
    selection betweeen the qsort and bucket sort.

GB_reduce_to_scalar: done, except needs a better terminal exit

----------------------------------------------------------------------
in progress
----------------------------------------------------------------------

Template/GB_qsort_template: IN PROGRESS
    Needs a way to set a larger leaf task size.
    Does lots of work, everywhere, for many GraphBLAS operations.
    Odd segfault with 20 threads.  see "TODO"

----------------------------------------------------------------------
not started:  C=A(I,J) and C(I,J)=A
----------------------------------------------------------------------

Template/GB_subref_template:
    Does the work for GrB_extract: A=C(I,J).  The list J can be partitioned and
    called in parallel to work on independent submatrices, like A1=C(I,J1)
    where J1 is the first set of columns in J.  Use a 2-phase method (phase1:
    count # entries, phase2: do the work).  Need to use fine tasks like
    GB_add, GB_emult, and GB_masker.  Note that J can have duplicates.

GB_subassign_kernel:
    This does C(I,J)=A (GrB_assign and GxB_subassign).  Huge file, but can be
    parallel since the main pass does not modify the pattern of C at all,
    except to kill entries by making them zombies.  If a new entry is to be
    added, it gets put into a list of pending tuples.  Each thread could make
    its own list, and the lists could be combined at the end.  This would be a
    single-phase method.

GB_assign:
    Does the work of GrB_*_assign.  Most of the work is in GB_subassign_kernel,
    but does work for the C_replace_phase, to create zombies in C outside of
    the IxJ submatrix.

GB_I_inverse:
    inverts the explicit list I for GB_subref_template.  A bucket sort; hard to
    parallelize the current algorithm.

GB_ijsort: deletes duplicates, see also GB_builder.  This is only used in
    GB_assign, for scalar expansion and for the C_replace_phase, and only when
    I and/or J are lists (not GrB_ALL, nor lo:inc:hi).

