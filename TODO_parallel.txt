GraphBLAS/TODO_parallel.txt: Here are a list of the Source/*.c files that need
to be done in parallel.  I've roughly sorted them by priority and partly by the
low-hanging fruit (some will be quick to do).  You can find them all with:

cd GraphBLAS/Source
grep -r PARALLEL .

There are just (!) 37 files to consider in all of GraphBLAS.  The other files
either call these, or do O(1) or O(log(..)) work and won't be done in parallel.

Testing the parallel codes:  Most of my test suite is done through MATLAB
mexFunctions in the GraphBLAS/Test directory, but that could be a hassle.
Better would be to use the algorithms and main programs in LAGraph (BFS for
now), GraphBLAS/Demo (BFS, MIS, PageRank, triangle counting) and
GraphBLAS/Extras (triangle counting, k-truss, and all-k-truss).  The k-truss
and all-k-truss do lots of matrix-matrix multiplies.  Pagerank is a good
test for mxv via "*FINE" slice methods.

We should probably write some unit tests for simpler functions like sorting and
transpose and such.  LAGraph would be the thing to use since it has a nice
LAGraph_mmread and LAGraph_mmwrite pair of functions.  Main programs in
GraphBLAS/Demo are more tedious without these nice functions.

The GB_AxB_parallel will use bulk parallelism with for loops that do
"for (id = 0 ; id < nthreads ; id++)", and then call GB* functions inside that
loop that then do not need to be parallel inside.  For other functions, we will
want to do parallel for loops for the GBI_for_each_vector(A) iterator.  I have
simplified this macro in this latest (Feb 16, 2019) version.  See the
GBI_* definitions in GB.h, and the #ifdef for_comments_only block.

The more complex iterators (GBI2, GBI2s, GBI3, GBI3s) can be ignored for now;
they iterate over a set union of 2 or more lists.  They are currently not
easily parallelizable and need to be replaced with the single GBI_ iterator if
the loop is to be parallelized.

----------------------------------------------------------------------

GB_AxB_parallel.c: IN PROGRESS: mxm nicely parallel for large matrices.
    This is where we do C=A*B, for GrB_mxm, GrB_mxv, and GrB_vxm.
    *FINE slice methods written but need tuning.
    Need better fine-grain methods for mxv and vxm:  do symbolic analysis
    phase (column nnz count), and then compute C in place without the final
    copy.

GB_AxB_flopcount.c: IN PROGRESS: reasonably implemented but needs a little work.
    Used by C=A*B for the parallel case, simple for all loop.  Note however the
    early exit, but this is done only when nthreads == 1.  Currently, one
    thread has a 'continue' statement but it could break early.

GB_hcat_slice.c: DONE
    easy; a simple for-all loop

Template/GB_qsort_template.c: IN PROGRESSS
    Needs a way to set a larger leaf task size.
    Does lots of work, everywhere, for many GraphBLAS operations.
    Odd segfault with 20 threads.  see "TODO"

GB_transplant.c:
    utility function used by almost all of GraphBLAS.  Easy to parallelize.

GB_cast_array.c:
    Typecasts the numerical values, so used everywhere. Easy to parallelize.

GB_add.c:
    C = A+B. Does the work for GrB_*_eWiseAdd, and the accumulator operator.
    Very important.  Need to break the work into symbolic/numeric phases.
    First phase only computes the number of entries in each column of C.

GB_mask.c:
    Does the C<M>=result for all operations.  Important function.  Parallel
    method would look like GB_add.

GB_emult.c:
    Does "C=A.*B" for GrB_*_eWiseMult_*.  Parallelism is a lot like GB_add.

GB_cumsum.c:
    a parallel cumulative-sum.  Used many places.
    Note the two variants.  Performance of the parallel case is at
    most 2x speedup, unless the array is already in L3 cache.
    needs more work.

GB_build.c, GB_builder.c, GB_build_factory.c:
    does the work of GrB_*_build.  Some is easy to do.  
    Also used by GB_transpose so it is an important function to parallelize.

GB_transpose.c:
    uses two methods, a qsort-based method and a bucket sort.
    The qsort should be done in parallel, but there are some for loops in
    this file that need to be parallel.

GB_reduce_to_scalar.c:
    Does the work for GrB_reduce, to a scalar.  Simple reduction for loop.
    Note that early exit for some monoids needs to be added.

GB_reduce_to_column.c:
    does the work for GrB_*_reduce (to a vector)

GB_extractTuples.c:
    does GrB_*_extractTuples.  Easy to parallelize

GB_apply_op.c.c:
    Applies a unary operator for GrB_*_apply.  Easy to parallelize.

Template/GB_prune_inplace.c:
    kills zombies for GrB_wait, and also to resize a matrix by GxB_*_resize.
    Could use a reduction-style parallelism.

GB_to_nonhyper.c, GB_to_hyper.c:
    converts to/from hypersparse and standard.  Important utility,
    easy to parallelize.

GB_wait.c:
    Does the work for GrB_wait for one matrix.  Kills zombies and assembles
    pending tuples.

GB_dup.c:
    does the work for GrB_*_dup.  Easy to parallelize.

GB_resize.c:
    does the work for GxB_*_resize.  Low priority but easy to do.

GB_calloc_memory.c:
    calls the ANSI C calloc function.  If the array is big, this could be
    set to zero in parallel.  Easy to do.

GB_realloc_memory.c:
    calls the ANSI C realloc function.  If the array is big, this could be done
    with a malloc, then a large parallel memcpy, then free the old space. 
    Easy to do.

GB_select.c:
    does the work for my GxB_*_select operations.  This prunes the matrix,
    like GB_prune_inplace.

GB_nvec_nonempty.c:
    counts # of non-empty vectors.  simple parallel reduction.
    Small utility routine used everywhere, but doesn't do a lot of work
    so lower priority.

GB_kron_kernel.c:
    should be easy to do.  Could just precompute the size of each column
    of the output matrix, quickly, and then to a parallel cumulative sum.
    Then fill the output.  But not as essential as the methods above.


----------------------------------------------------------------------
For C=A(I,J) and A(I,J)=C:  some are hard to do
----------------------------------------------------------------------

GB_assign.c:
    Does the work of GrB_*_assign.
    This will be hard, but not impossible.
    Most of the work is in GB_subassign_kernel

Template/GB_subref_template.c:
    Does the work for A=C(I,J).  The list J can be partitioned and this can
    be called in parallel to create independent submatrices, like
    A1 = C(I,J1) where J1 is the first set of columns in J.
    Then the results can be concatenated together, like C=A*B.

GB_subassign_kernel.c:
    This does C(I,J)=A.  Huge file, but could be done in parallel since the
    main pass does not modify the pattern of C at all, except to kill
    entries by making them zombies.  If a new entry is to be added, it gets
    put into a list of pending tuples.  Each thread could make its own list,
    and the lists could be combined when done.

GB_I_inverse.c, GB_ijproperties.c, GB_ijsort.c:
    utility functions for assign and extract (A=C(I,J) and C(I,J)=A).
    I_inverse is hard to parallelize.


----------------------------------------------------------------------
bucket transpose: don't parallelize at all?
----------------------------------------------------------------------

GB_transpose_ix, GB_transpose_op, GB_transpose_bucket
    for the transpose bucket sort.  hard to parallelize but we can
    use the qsort method instead of the bucket method.  So this is low
    priority, if at all.

