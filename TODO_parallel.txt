GraphBLAS/TODO_parallel.txt: Here are a list of the Source/*.c files that need
to be done in parallel.  I've roughly sorted them by priority and partly by the
low-hanging fruit (some will be quick to do).  You can find them all with:

cd GraphBLAS/Source
grep -r PARALLEL .

Testing the parallel codes:  Most of my test suite is done through MATLAB
mexFunctions in the GraphBLAS/Test directory.  The mexFunctions link against
the GraphBLAS library, so do a "make install" and then use Test/testall.m
in MATLAB.

Alternatively: use the algorithms and main programs in LAGraph (BFS for now),
GraphBLAS/Demo (BFS, MIS, PageRank, triangle counting) and GraphBLAS/Extras
(triangle counting, k-truss, and all-k-truss).  The k-truss and all-k-truss do
lots of matrix-matrix multiplies.  Pagerank is a good test for mxv via "*FINE"
slice methods.  For the parallel qsort, use LAGraph/BuildMatrix/btest to test
the parallel performance.

A few methods use the GBI_parallel_for_each_vector(A) iterator, but most use
their own parallel for loop structure.  The more complex iterators (GBI2,
GBI2s, GBI3, GBI3s) can be ignored for now; they iterate over a set union of 2
or more lists.  They are currently not easily parallelizable and need to be
replaced with simpler loops to parallelize them.  See for example the new
GB_add.c.

All parallel methods need tuning.  Right now, I do not reduce nthreads for
smaller problems, and so they can see significant slowdowns (C=A*B when the
matrices are 16-by-16 with 16 threads is very slow).  All parallel regions use
the num_threads(nthreads) construct.  Need to write this instead:

    int nth = GB_nthreads_workload (c*work, nthreads) ;
    #pragma omp parallel for num_threads(nth)
    for ( ... )

where "work" is the total work of the parallel for loop or region.
It should be O(actual work), with a constant c that will vary per
for loop.  Then decide on a chunk size (perhaps in a global parameter)
that does something like:

    int GB_nthreads_workload (double work, int nthreads)
    {
        return (max (nthreads, 1 + floor (work / chunk))) ;
    }

----------------------------------------------------------------------
DONE or mostly done:
----------------------------------------------------------------------

GB_AxB_parallel.c: much is done: mxm nicely parallel for large matrices,
    and decent parallelism for mxv when using the dot product.  This is where
    we do C=A*B, for GrB_mxm, GrB_mxv, and GrB_vxm.  *FINE slice methods are
    written but need tuning.  Need better fine-grain methods for mxv and vxm:
    do symbolic analysis phase (column nnz count), and then compute C in place
    without the final copy.  See for example *dot2.c. Need to do the same for
    Gustavson's method and for the Heap method.  Need to add the SLICE methods
    to the descriptor.  Need to write a hash-based method.

GB_AxB_flopcount.c: mostly done: reasonably implemented but needs a little work.
    Used by C=A*B for the parallel case, simple for all loop.  Note however the
    early exit, but this is only when nthreads == 1.  Currently, one thread has
    a 'continue' statement but if any thread terminates then all threads could
    break early.

GB_transplant.c: mostly done.
    utility function used by almost all of GraphBLAS.  Does all its work in the
    parallel GB_memcpy.  But could use OpenMP tasks to do all GB_memcpy's in
    parallel.

GB_cast_array.c: done.
    Typecasts the numerical values, so used everywhere.

GB_add.c: done, except for final prune of C->h.

GB_emult.c: done, except for final prune of C->h.

GB_mask.c: done, except for final prune of C->h

GB_cumsum.c: done, but not as fast as I think it should be.
    a parallel cumulative-sum.  Used many places.  Note the two variants.
    Performance of the parallel case is at most 2x speedup, unless the array is
    already in L3 cache.  Needs more work.

GB_transpose.c: done, except for the qsort.  uses two methods, a qsort-based
    method and a bucket sort.  Once the qsort is done, need to tune the auto
    selection betweeen the qsort and bucket sort.

GB_transpose_ix, GB_transpose_op, GB_transpose_bucket: done.

GB_reduce_to_scalar.c: done.
    Does the work for GrB_reduce, to a scalar.  Simple reduction for loop.

GB_extractTuples.c: done, except tasks can be used to extract I,J,X in parallel

GB_apply_op.c: done.  Applies a unary operator for GrB_*_apply.

GB_wait.c: done, except it will be updated when GB_add can tolerate zombies
    on input

GB_dup.c:  done.  4 parallel memcpy's, but these can also be 4 tasks.
    does the work for GrB_*_dup.  Easy to parallelize.

GB_build.c, GB_builder: done.
    does GrB_*_build, GB_wait, GrB_reduce_to_vector (transposed),
    and GrB_transpose (when the qsort method is used).

GB_reduce_to_vector.c: done.
    does the work for GrB_*_reduce (to a vector)

GB_select.c, GB_selector.c: done.

----------------------------------------------------------------------
in progress
----------------------------------------------------------------------

Template/GB_qsort_template.c: IN PROGRESS
    Needs a way to set a larger leaf task size.
    Does lots of work, everywhere, for many GraphBLAS operations.
    Odd segfault with 20 threads.  see "TODO"

----------------------------------------------------------------------
not started
----------------------------------------------------------------------

GB_to_nonhyper.c, GB_to_hyper.c:
    converts to/from hypersparse and standard.  Important utility, but does
    only O(n) work if n = # vectors in the matrix.  Parallel speedup will be
    low unless the problem is huge.

GB_nvec_nonempty.c:
    counts # of non-empty vectors.  simple parallel reduction.
    Small utility routine used everywhere, but doesn't do a lot of work
    so lower priority.

GB_is_diagonal: determines if A is diagonal, for C=A*B.  Very little work to
    do, and can exit early as soon as it finds a vector k so that A(:,k)
    contains anything more than a single A(k,k) entry.  So it really needs
    a parallel early-exit.

GB_kron_kernel.c:
    Easy, with a 2-phase method.

----------------------------------------------------------------------
not started:  C=A(I,J) and C(I,J)=A
----------------------------------------------------------------------

GB_ijproperties.c: easy
    determines the properties of I and J for GB_subref_template and
    GB_subassign_kernel.

Template/GB_subref_template.c:
    Does the work for GrB_extract: A=C(I,J).  The list J can be partitioned and
    called in parallel to work on independent submatrices, like A1=C(I,J1)
    where J1 is the first set of columns in J.  Use a 2-phase method (phase1:
    count # entries, phase2: do the work).  Need to use fine tasks like
    GB_add, GB_emult, and GB_masker.  Note that J can have duplicates.

GB_subassign_kernel.c:
    This does C(I,J)=A (GrB_assign and GxB_subassign).  Huge file, but can be
    parallel since the main pass does not modify the pattern of C at all,
    except to kill entries by making them zombies.  If a new entry is to be
    added, it gets put into a list of pending tuples.  Each thread could make
    its own list, and the lists could be combined at the end.  This would be a
    single-phase method.

GB_assign.c:
    Does the work of GrB_*_assign.  Most of the work is in GB_subassign_kernel,
    but does work for the C_replace_phase, to create zombies in C outside of
    the IxJ submatrix.

GB_I_inverse.c:
    inverts the explicit list I for GB_subref_template.  A bucket sort; hard to
    parallelize the current algorithm.

GB_ijsort: deletes duplicates, see also GB_builder.  This is only used in
    GB_assign, for scalar expansion and for the C_replace_phase, and only when
    I and/or J are lists (not GrB_ALL, nor lo:inc:hi).

