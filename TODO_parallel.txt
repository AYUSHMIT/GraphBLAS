GraphBLAS/TODO_parallel.txt: Here are a list of the Source/*.c files that need
to be done in parallel.  I've roughly sorted them by priority and partly by the
low-hanging fruit (some will be quick to do).  You can find them all with:

cd GraphBLAS/Source
grep -r PARALLEL .

There are just (!) 36 files to consider in all of GraphBLAS.  The other files
either call these, or do O(1) or O(log(..)) work and won't be done in parallel.

Testing the parallel codes:  Most of my test suite is done through MATLAB
mexFunctions in the GraphBLAS/Test directory, but that could be a hassle.
Better would be to use the algorithms and main programs in LAGraph (BFS for
now), GraphBLAS/Demo (BFS, MIS, PageRank, triangle counting) and
GraphBLAS/Extras (triangle counting, k-truss, and all-k-truss).  The k-truss
and all-k-truss do lots of matrix-matrix multiplies.  Pagerank is a good
test for mxv via "*FINE" slice methods.

We should probably write some unit tests for simpler functions like sorting and
transpose and such.  LAGraph would be the thing to use since it has a nice
LAGraph_mmread and LAGraph_mmwrite pair of functions.  Main programs in
GraphBLAS/Demo are more tedious without these nice functions.
For the parallel qsort, use LAGraph/BuildMatrix/btest to test the parallel
performance.

See the new GBI_parallel_for_each_vector(A) iterator.
The more complex iterators (GBI2, GBI2s, GBI3, GBI3s) can be ignored for now;
they iterate over a set union of 2 or more lists.  They are currently not
easily parallelizable and need to be replaced with the single GBI_ iterator if
the loop is to be parallelized.  See for example the new GB_add_phase*.c.

----------------------------------------------------------------------

GB_AxB_parallel.c: IN PROGRESS: mxm nicely parallel for large matrices.
    This is where we do C=A*B, for GrB_mxm, GrB_mxv, and GrB_vxm.
    *FINE slice methods written but need tuning.
    Need better fine-grain methods for mxv and vxm:  do symbolic analysis
    phase (column nnz count), and then compute C in place without the final
    copy.  See for example *dot2.c. Need to do the same for Gustavson's
    method and for the Heap method.

GB_AxB_flopcount.c: IN PROGRESS: reasonably implemented but needs a little work.
    Used by C=A*B for the parallel case, simple for all loop.  Note however the
    early exit, but this is done only when nthreads == 1.  Currently, one
    thread has a 'continue' statement but it could break early.

Template/GB_qsort_template.c: IN PROGRESSS
    Needs a way to set a larger leaf task size.
    Does lots of work, everywhere, for many GraphBLAS operations.
    Odd segfault with 20 threads.  see "TODO"

GB_transplant.c:
    utility function used by almost all of GraphBLAS.  Easy to parallelize;
    does all its work in the parallel GB_memcpy.  But could use OpenMP tasks to
    do all GB_memcpy's in parallel.

GB_cast_array.c: DONE
    Typecasts the numerical values, so used everywhere. Easy to parallelize.

GB_add.c:
    C = A+B. Does the work for GrB_*_eWiseAdd, and the accumulator operator.
    Very important.  GB_add_phase* breaks the work into symbolic/numeric
    phases.  phase0 finds the columns of C, phase1 computes the number of
    entries in each column of C. phase2 does the work.  Need to keep the
    old GB_add for single-thread case (commented out for now).

GB_mask.c:
    Does the C<M>=result for all operations.  Important function.  Parallel
    method would look like GB_add_phase*

GB_emult.c:
    Does "C=A.*B" for GrB_*_eWiseMult_*.  Parallelism is a lot like
    GB_add_phase*

GB_cumsum.c: written but not as fast as I think it should be.
    a parallel cumulative-sum.  Used many places.
    Note the two variants.  Performance of the parallel case is at
    most 2x speedup, unless the array is already in L3 cache.
    needs more work.

GB_build.c: done, except the internal loop needs vectorization

GB_builder.c, GB_build_factory.c:
    does the work of GrB_*_build.  Some is easy to do.  
    Also used by GB_transpose so it is an important function to parallelize.

GB_transpose.c: done (see bucket sort elsewhere)
    uses two methods, a qsort-based method and a bucket sort.

GB_reduce_to_scalar.c: done
    Does the work for GrB_reduce, to a scalar.  Simple reduction for loop.
    Note that early exit for some monoids needs to be added.

GB_reduce_to_column.c:
    does the work for GrB_*_reduce (to a vector)

GB_extractTuples.c: done, except tasks can be used to extract I,J,X in parallel
    does GrB_*_extractTuples.

GB_apply_op.c.c: done
    Applies a unary operator for GrB_*_apply.  Easy to parallelize.

Template/GB_prune_inplace.c:
    kills zombies for GrB_wait, and also to resize a matrix by GxB_*_resize.
    Could use a reduction-style parallelism.

GB_to_nonhyper.c, GB_to_hyper.c:
    converts to/from hypersparse and standard.  Important utility,
    easy to parallelize.

GB_wait.c:
    Does the work for GrB_wait for one matrix.  Kills zombies and assembles
    pending tuples.  This is slow compared to the parallel GB_add_phase*.
    Consider replacing it with GB_add_phased.

GB_dup.c:  4 parallel memcpy's, but these can also be done in 4 tasks
    does the work for GrB_*_dup.  Easy to parallelize.

GB_resize.c:
    does the work for GxB_*_resize.  Low priority but easy to do.

GB_select.c:
    does the work for my GxB_*_select operations.  This prunes the matrix,
    like GB_prune_inplace.

GB_nvec_nonempty.c:
    counts # of non-empty vectors.  simple parallel reduction.
    Small utility routine used everywhere, but doesn't do a lot of work
    so lower priority.

GB_kron_kernel.c:
    should be easy to do.  Could just precompute the size of each column
    of the output matrix, quickly, and then to a parallel cumulative sum.
    Then fill the output.  But not as essential as the methods above.


----------------------------------------------------------------------
For C=A(I,J) and A(I,J)=C:  some are hard to do
----------------------------------------------------------------------

GB_assign.c:
    Does the work of GrB_*_assign.
    This will be hard, but not impossible.
    Most of the work is in GB_subassign_kernel

Template/GB_subref_template.c:
    Does the work for A=C(I,J).  The list J can be partitioned and this can
    be called in parallel to create independent submatrices, like
    A1 = C(I,J1) where J1 is the first set of columns in J.
    Then the results can be concatenated together, like C=A*B.

GB_subassign_kernel.c:
    This does C(I,J)=A.  Huge file, but could be done in parallel since the
    main pass does not modify the pattern of C at all, except to kill
    entries by making them zombies.  If a new entry is to be added, it gets
    put into a list of pending tuples.  Each thread could make its own list,
    and the lists could be combined when done.

GB_I_inverse.c, GB_ijproperties.c, GB_ijsort.c:
    utility functions for assign and extract (A=C(I,J) and C(I,J)=A).
    I_inverse is hard to parallelize.


----------------------------------------------------------------------
bucket transpose: don't parallelize at all?
----------------------------------------------------------------------

GB_transpose_ix, GB_transpose_op, GB_transpose_bucket
    for the transpose bucket sort.  hard to parallelize but we can
    use the qsort method instead of the bucket method.  So this is low
    priority, if at all.

